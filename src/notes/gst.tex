\section{Gate Set Tomography}

Notes taken from \cite{nielsen_gate_2020}.

\todoin{In the final dissertation, the precise mathematics of quantum state, process, and
measurement tomography are not important. Remove these bits and trim down the in-depth detail of
LGST and LSGST. See presentation slides for required information.}

\noindent \ac{GST} differs from state and process tomography in that:
\begin{itemize}
    \item It is almost entirely calibration-free. It does not depend upon a prior description of the
    measurements used (as in state tomography) or the states that can be prepared (as in process
    tomography). \textit{These are called ``reference frame'' operations in the literature?}
    \item It estimates an entire set of logic operations, rather than a single one.
\end{itemize}

\ac{GST} being calibration-free is incredibly important. Both state and process tomography are
limited in that they rely on \textit{accurate} characterisation of their ``reference frame''
operations. Typically, they're either unknown or misidentified.

\subsection{Mathematical Background}

% The important part for me :(

A quantum system is described by a $d$-dimensional \textit{Hilbert space} $\mathcal{H} =
\mathbb{C}^d$, where $d$ is the largest number of outcomes of a repeatable measurement. For a qubit,
$d = 2$. GST uses the \textit{Hilbert-Schmidt space}. The Hilbert-Schmidt space is the complex
$d^2$-dimensional vector space of $d \times d$ matrices. We're interested in the $d^2$-dimensional
subspace of Hermitian matrices, denoted $\mathcal{B(H)}$. The basis we use for $\mathcal{B(H)}$ is
the set of normalised Pauli matrices $\{ \mathbb{I} / \sqrt{2}, \sigma_x / \sqrt{2}, \sigma_y /
\sqrt{2}, \sigma_z / \sqrt{2} \}$. This basis has the following properties:
\begin{itemize}
    \item Hermicity: $B_i = B_i^{\dagger}$
    \item Orthonormality: $\tr{B_i B_j} = \delta_{ij}$
    \item Traceless for $i > 0$: $B_0 = \mathbb{I} / \sqrt{d}$ and $\tr{B_i} = 0 \enspace \forall{i}
    > 0$.
\end{itemize}

Elements of $\mathcal{B(H)}$ are represented using an `extension' of Dirac's bra-ket notation called
\textit{super bra-ket notation}. Some element $B$ is represented as a column vector $\superket{B}$,
and an element of its dual space by a row vector $\superbra{A}$. Everything works similarly to
regular Dirac notation, the main difference is that we can represent everything as vectors in
$\mathcal{B(H)}$ rather than the usual matrices.

Measurement of a quantum system yields an outcome from a set of $k$ possibilities. Therefore, the
$i$th outcome can be represented by a dual vector $\superbra{E_i}$, so that $\Pr(i | \rho) =
\superbraket{E_i | \rho} = \tr{E_i \rho}$. Since they represent probabilities, we require that $E_i
\ge 0$ and $\sum_i E_i = \mathbb{I}$. The $E_i$ are called \textit{effects}, and the set $\{E_i \}$
is called a \ac{POVM}.  Note that since both states and effects are both Hermitian, we can in fact
represent them in the $d^2$-dimensional real subspace of $\mathcal{B(H)}$. Therefore, any reference
to $\mathcal{B(H)}$ is referring to the real subspace.

\subsection{Quantum Logic Gates}

An \textit{ideal} quantum logic gate is \textit{reversible} and corresponds to a unitary transform
of $\mathcal{H}$. Such a gate would transform $\rho$ as $\rho \to U \rho U^{\dagger}$ for some
unitary matrix $U$. This is a linear transformation from $\mathcal{B(H)}$ to itself; the linear
transformation $\rho \to U \rho U^{\dagger}$ is called a \textit{superoperator}. In reality, logic
gates are not perfectly reversible. These superoperators are known as quantum processes or quantum
channels. We can represent any superoperator $\Lambda$ as a $d^2 \times d^2$ matrix, which acts on
$\superket{\rho} \in \mathcal{B(H)}$ by left multiplication. This representation is called the
\textit{transfer matrix} of $\Lambda$, and is denoted by $S_{\Lambda}$. Thus,
\begin{equation}
    \Lambda : \superket{\rho} \mapsto S_{\Lambda} \superket{\rho}
\end{equation}
If $\Lambda$ is performed on some input state $\rho$, then the probability of outcome $E_i$ is
therefore
\begin{equation}
    p_i = \superbraket{E_i | S_{\Lambda} | \rho} = \tr{E_i S_{\Lambda} \rho}
\end{equation}

Not all superoperators describe physical operations. To be physically possible, they must be:
\begin{itemize}
    \item \textit{Trace-preserving}: $\tr{\Lambda(\rho)}$ must equal 1 for all $\rho$.
    \item \textit{Completely Positive}: when $\Lambda$ acts on part of a larger system, it must
    preserve positivity for the entire system. A superoperator is \textit{positive} iff.
    $\Lambda(\rho) \ge 0$ for all $\rho$. A superoperator is \textit{completely positive} iff.
    $\Lambda \otimes \mathbb{I}_{\mathcal{A}}$ is positive for any auxiliary state space
    $\mathcal{A}$.
\end{itemize}
This \ac{CPTP} constraint alone is sufficient -- any \ac{CPTP} superoperator can be physically
implemented. The TP condition corresponds to $\superbra{\mathbb{I}} S_{\Lambda} =
\superbra{\mathbb{I}}$. Since our basis is traceless for $i > 0$, then $\Lambda$ is TP iff. the
first row of $S_{\Lambda}$ is $[1, 0, \dots, 0]$. The CP condition is a lot more tricky to describe.
We first rewrite $S_{\Lambda}$ in the operator-sum representation:
\begin{equation}
    \Lambda : \rho \mapsto \sum_{ij} \chi_{ij}^{\Lambda} B_i \rho B_j^{\dagger}
\end{equation}
where $\{B_i\}$ is a basis, and $\chi_{ij}^{\Lambda}$ is a matrix of coefficients called the ``Choi
process matrix'' which represents $\Lambda$. Similarly to the chi matrix representation from
earlier, this completely describes $\Lambda$ (read
\href{https://quantumcomputing.stackexchange.com/a/11814}{this answer} for more about their
relationship). The mapping between $S_{\Lambda}$ and $\chi_{ij}^{\Lambda}$ is known as the
Choi-JamioÅ‚kowski isomorphism:
\begin{equation}
    \chi^{\Lambda} = d(S_{\Lambda} \otimes \mathbb{I}) \superket{\Pi_{\text{EPR}}}
\end{equation}
where $\superket{\Pi_{\text{EPR}}}$ is a maximally entangled state. Note that the equality above
really means element-wise equality in a consistent basis. This is all quite complex. Fortunately,
the process is simple once we have the Choi process matrix: $\Lambda$ is CP iff. $\chi$ is positive
semidefinite. 
\todoin{Research Choi representation vs. Chi representation}

\subsection{Gate Sets}

A quantum processor's capabilities can be specified with a \textit{gate set}. Consider a processor
that can perform:
\begin{itemize}
    \item $N_G$ distinct gates,
    \item $N_{\rho}$ distinct state preparations,
    \item $N_M$ distinct measurements, with $N_E^{(m)}$ distinct outcomes.
\end{itemize}
We can use these to construct a gate set
\begin{equation}
    \mathcal{G} = 
    \left\{
        \left\{
            \superket{\rho^{(i)}}
        \right\}^{N_{\rho}}_{i=1} ;
        \left\{
            G_i
        \right\}^{N_G}_{i=1} ;
        \left\{
            \superbra{E_i^{(m)}}
        \right\}^{N_m, N_E^{(m)}}_{m=1, i=1}
    \right\}
\end{equation}
where
\begin{equation}
    \begin{aligned}
        G_i : \mathcal{B(H)} \to \mathcal{B(H)}  \quad &\text{for} \quad i = 1, \dots, N_G \\
        \superket{\rho^{(i)}} \in \mathcal{B(H)} \quad &\text{for} \quad i = 1, \dots, N_{\rho} \\
        \superbra{E_i^{(m)}} \in \mathcal{B(H)}^*  \quad &\text{for} \quad m = 1, \dots, N_M, i = 1, \dots, N_E^{(m)} \\
    \end{aligned}
\end{equation}
In other words, we can describe the capabilities of a quantum processor by collecting together the
possible `input states', the quantum gates, and the possible measurement outcomes for each input
state.

The operations that a gate set describes are related to each other. In fact, the specification given
above is an overspecification of the gate set. Consider a transformation of the gate set that acts
as
\begin{equation} \label{eq:gauge_freedom}
    \begin{aligned}
        \superbra{E_i^{(m)}}  &\to \superbra{E_i^{(m)}} M^{-1} \\
        \superket{\rho^{(i)}} &\to M \superket{\rho^{(i)}} \\
        G_i                   &\to M G_i M^{-1}
    \end{aligned}
\end{equation}
where $M$ is an invertible superoperator. Although this changes the gate set, it does not change the
observed probabilities (see Equation \ref{eq:observed_freq}). This means that gate sets in fact
describe a family of equivalent gate sets. This degeneracy is known as \textit{gauge freedom}.

\subsection{Circuits}

The term `quantum circuit' can refer to different things. There are two related but distinct types
of quantum circuit that we're interested in.
\begin{itemize}
    \item \textbf{Fixed-Input, Classical-Output (FI/CO)}: a quantum circuit where each experiment is
    described by a quantum circuit that begins by initialising and ends by measuring all of the
    qubits. A FI/CO circuit describes a probability distribution over classical bit strings.
    \item \textbf{Quantum-Input, Quantum-Output (QI/QO)}: a quantum circuit which is an arrangement
    of unitary logic gates, with no explicit initialisation or measurement. This may be inserted
    into a large quantum circuit as a `sub-routine'.
\end{itemize}

QI/QO circuits are defined as a sequence of layers $S = (\gamma_1, \gamma_2, \dots, \gamma_L)$. Each
layer corresponds to applying some superoperator $G_{\gamma_i}$. The entire QI/QO circuit $S$ also
corresponds to applying a superoperator. We denote the transfer matrix for $S$ by $\tau(S)$, where
\begin{equation}
    \tau(S) = \tau((\gamma_1, \gamma_2, \dots, \gamma_L)) = G_{\gamma_L} \dots G_{\gamma_2} G_{\gamma_1}
\end{equation}
which is formed by composing the elements from each layer. Exponentiation of a circuit describes
repetition, and since $S^2 = SS$, it follows that
\begin{equation}
    \tau(S^n) = \tau(S)^n
\end{equation}

For a set of FI/CO circuits, we can generate data sets by repeating each one $N$ times, and
recording the results. The results are summarized by observed frequencies, $f_k = n_k / N$, which
approaches the corresponding probabilities
\begin{equation} \label{eq:observed_freq}
    f_k \approx \superbraket{E_k^{(m)} | G_{\gamma_L} \dots G_{\gamma_2} G_{\gamma_1} | \rho^{(i)}}
\end{equation}
and can be used to estimate them. This is not the only way to do so, but it illustrates that we can
infer some of $\superket{\rho^{(i)}}$, $\superbra{E_k^{(m)}}$, and $G_j$ based on observed
frequencies. This is called tomography. \textit{Each type of tomography treats some of the
operations as known, and uses them as the reference frame to estimate the others}.

\subsection{Tomography}

\subsubsection{Quantum State Tomography}

In order to perform \textit{any} type of tomography, we must have a \textit{fiducial} and
\textit{informationally complete} set. ``Fiducial'' means that it is accepted as a fixed basis of
reference, and ``informationally complete'' means that it will uniquely identify any target
information. 

In the context of state tomography, we're aiming to describe some unknown quantum state $\rho$,
given some fiducial and informationally complete measurements. In other words, the measurements are
a frame of reference and must uniquely identify $\rho$. This implies that the set of measurements
$\{E_i^{(m)}\}_{m,i}$ must span the entire space of effects. To perform state tomography, many
copies of $\rho$ are made available, and divided into $M$ pools. The $m$th fiducial measurement is
applied to all the copies in the $m$th pool, and used to estimate the probabilities,
\begin{equation}
    p_i^{(m)} (\rho) = \tr{\rho E_i^{(m)}}
\end{equation}
which should uniquely identify $\rho$. In general, this is only true if we have infinite copies of
$\rho$. In practice, we have limited numbers of $\rho$, and therefore $\hat{p}_i = f_i$ where $f_i$
are the frequencies for each measurement. This often yields an estimate $\hat{\rho}$ which is not
positive. Nevertheless, it is still useful.

Let's assume that we have the exact probabilities for each measurement outcome for the state $\rho$.
We can in fact ignore the measurements themselves, what is important is the list of effects, so we
can write the set simply as $\{ E_j : j = 1, \dots, N_{f1}\}$ where $f1$ is the total number of
distinct measurement outcomes. Additionally, we can represent these as dual vectors $\superbra{E_j}$
in $\mathcal{B(H)}$ just like before. To recover $\rho$, we can write Born's rule as an inner
product, 
\begin{equation}
    \begin{aligned}
        p_j &= \tr{E_j \rho} \\
            &= \superbraket{E_j | \rho}
    \end{aligned}
\end{equation}
Then, we can stack all of the effects into a single $N_{f1} \times d^2$ matrix
\begin{equation}
    A = \begin{bmatrix}
        \superbra{E_1} \\
        \superbra{E_2} \\
        \vdots \\
        \superbra{E_{N_{f1}}}
    \end{bmatrix}
\end{equation}
which gives $\vec{p} = A \superket{\rho}$. If $A$ is square, then we can recover $\rho$ with:
\begin{equation}
    \superket{\rho} = A^{-1} \vec{p}
\end{equation}
If $N_{f1}$ is greater than $d^2$, making $A$ non-square, we must solve with a pseudo-inverse:
\begin{equation}
    \begin{aligned}
        (A^T A)^{-1} A^T \vec{p} &= (A^T A)^{-1} A^T A \superket{\rho} \\
                                 &= (A^T A)^{-1} (A^T A) \superket{\rho} \\
                                 &= I \superket{\rho} \\
                                 &= \superket{\rho}
    \end{aligned}
\end{equation}

\subsubsection{Quantum Process Tomography}

In the context of process tomography, we're aiming to describe some quantum process (e.g. a quantum
gate), given an informationally complete set of known fiducial states. Broadly,
we prepare many copies of them, pass them through the target process, and perform state tomography
on the output states.

Let $G$ be the superoperator representing the process we want to reconstruct. If state $\rho_i$ is
prepared, $G$ is applied, and measurement is performed with possible outcomes $\{E_j\}$, then the
probability of observing $E_j$ is
\begin{equation}
    \begin{aligned}
        P{j,i} &= \tr{E_j G[\rho_i]} \\
               &= \superbraket{E_j | G | \rho_i}
    \end{aligned}
\end{equation}
We can then define a $d^2 \times N_{f2}$ matrix $B$, similarly to $A$, which represents the fiducial
states $\superket{\rho_i}$:
\begin{equation}
    B = 
    \begin{bmatrix}
        \superket{\rho_1} & \superket{\rho_2} & \dots & \superket{\rho_{N_{f2}}}
    \end{bmatrix}
\end{equation}
which gives the $N_{f1} \times N_{f2}$ matrix $P = AGB$. Similarly to before, we can recover $G$
with (pseudo)-inverses. If $P$ is square, we can recover $G$ with:
\begin{equation}
    G = A^{-1} P B^{-1}
\end{equation}
If $P$ is non-square, then again we use a pseudo-inverse:
\begin{equation}
    G = (A^T A)^{-1} A^T P B^T (B B^T)^{-1}
\end{equation}

\subsubsection{Calibration}

The requirements of fiducial states/measurements in state and process tomography show why GST being
calibration-free is important. In practice, we never have access to perfectly known
states/measurements, and they're also not noiseless. In order to identify the exact fiducial
measurements for state tomography, we would need perfectly known states, which would require state
tomography.  Similarly, process tomography relies on fiducial states and measurements which are
almost always produced by applying quantum logic gates. Identifying these would again require
process tomography. It's an endless loop of self-referentiality. In realistic scenarios, errors in
state preparation and measurement (SPAM) dominate inaccuracy in process tomography.

\subsection{Linear Gate Set Tomography (LGST)}

\ac{LGST} looks very much like process tomography, but is doing something significantly different.
Unlike process tomography, it reconstructs the entire gate set up to \textit{gauge freedom} as shown
in Equation \ref{eq:gauge_freedom}. Quantum operations are usually described relative to an implicit
and absolute reference frame. But in most experiments, no such reference frame is available. GST
characterises all of these operations \textit{relative to each other}, and \textit{estimates} every
property of a gate set that can be measured without a reference frame. Those that cannot be measured
without a reference frame correspond to gauge degrees of freedom.

Because of gauge freedom, the representation produced by \ac{LGST} is generally not unique. Another
shortcoming of \ac{LGST} is its similarity to process tomography. $N$ trials of an event with
probability $p$ generally yields $\hat{p} = p \pm O(1 / \sqrt{N})$, meaning that errors scale with
$O(1 / \sqrt{N})$ like process tomography. Therefore, in order to estimate a gate set to within $\pm
10^{-5}$ would require repeating each circuit $N \approx 10^{10}$ times, which is impractical.

% not very note-like anymore is it? Turning into the full on thesis atm :D

\subsubsection{LGST Algorithm}

We make some assumptions first, which we relax later:
\begin{itemize}
    \item We can create informationally complete sets of fiducial states and measurement effects.
    However, we do not know them.
    \item We ignore finite sample error.
    \item We assume that the fiducial states and effects are \textit{exactly} informationally
    complete, giving $N_{f1} = N_{f2} = d^2$.
\end{itemize}

Similarly to process tomography, to reconstruct the set of gates $\{G_k\}$, we require a matrix
$P_k$ for each gate:
\begin{equation}
    [P_k]_{i,j} = \superbraket{E_i' | G_k | \rho_j'}
\end{equation}
We don't know what $\rho_j'$ and $E_i'$ are, but we are able to prepare them, meaning that we can
measure the probabilities that they produce. As before, we have
\begin{equation}
    P_k = A G_k B
\end{equation}
but \textit{we do not know what $A$ or $B$ are}. So it may seem that we cannot solve for $G_k$.
Instead, we measure some additional probabilities that correspond to tomography on the \textit{null
operation} which will take our ignorance about $\rho_j'$ and $E_i'$ into account. We arrange these
into a Gram matrix
\begin{equation}
    \tilde{\mathbbm{1}}_{i, j} = \superbraket{E_i' | \rho_j'}
\end{equation}
which is in fact $\tilde{\mathbbm{1}} = AB$. Then we can solve for $G_k$ with
\begin{equation}
    G_k = B \tilde{\mathbbm{1}}^{-1} P_k B^{-1}
\end{equation}
To reconstruct the states $\rho^{(l)}$ and measurement effects $\{E_l^{(m)}\}$, we can similarly
construct vectors of observable probabilities:
\begin{equation}
    \begin{aligned}
        \left[\vec{R}^{(l)}\right]_j &= \superbraket{E_j' | \rho^{(l)}} \\
        \left[\vec{Q}^{(m)}\right]_j &= \superbraket{E_l^{(m)} | \rho_j'} \\
    \end{aligned}
\end{equation}
Measuring these probabilities corresponds to state tomography on each native state $\rho^{(l)}$, and
measurement tomography on every native effect $\{E_l^{(m)}\}$. They can be written in terms of $A$
and $B$ as
\begin{equation}
    \begin{aligned}
        \vec{R}^{(l)} &= A \superket{\rho^{(l)}} \\
        \vec{Q}_l^{(m)T} &= \superbraket{E_l^{(m)} | \rho_j'} \\
    \end{aligned}
\end{equation}
which we can sub $\tilde{\mathbbm{1}} = AB$ into to get:
\begin{align}
    G_k                   &= B \tilde{\mathbbm{1}}^{-1} P_k B^{-1} \\
    \superket{\rho^{(l)}} &= B \tilde{\mathbbm{1}}^{-1} \vec{R}^{(l)} \\
    \superbra{E_l^{(m)}}  &= \vec{Q}_l^{(m)T} B^{-1}
\end{align}
\textit{This has recovered the original gate set up to gauge freedom}! The best choice for $B$
requires \textit{a posteriori} gauge-fixing.
\todoin{Look at gauge-fixing}

\subsubsection{Over-Completeness}

We assumed that $N_{f1} = N_{f2} = d^2$, but this isn't always the case. This means that $A$, $B$
and $P_k$ are generally not square and invertible. Additionally, due to finite sample errors,
\begin{equation}
    \begin{aligned}
        P_k                 &= A G_k B \\
        \tilde{\mathbbm{1}} &= AB
    \end{aligned}
\end{equation}
may not have exact solutions. Instead we can find an approximate solution with a least-squares
estimator, i.e. we want to minimise $|P_k - A G_k B|^2$ and $|\tilde{\mathbbm{1}} - AB|^2$. For
$|\tilde{\mathbbm{1}} - AB|^2$, using differentiation yields
\begin{equation} \label{eq:min_i_tilde}
    A = \tilde{\mathbbm{1}} B^T (B B^T)^{-1}
\end{equation}
We can do the same for $|P_k - A G_k B|^2$ and solve for $G_k$ with pseudo-inverses, yielding
\begin{equation}
    G_k = (A^T A)^{-1} A^T P_k B^T (B B^T)^{-1}
\end{equation}
and substituting $A$ from before yields
\begin{equation}
    G_k = B \left[ B^T \left(B \tilde{\mathbbm{1}}^T \tilde{\mathbbm{1}}\right)^{-1} B\right]
    \tilde{\mathbbm{1}}^T P_k \left[B^T (B B^T)^{-1}\right]
\end{equation}
This is just a generic version of the formulation from before, \textit{however}, $B$ is no longer
assumed to be square and invertible. In this case, this isn't true, which means that $B$ does affect
the probability estimates, but \textit{only through its support}. $B$ has dimensions $d^2 \times
N_{f1}$, but since the fiducial states are informationally complete, then its rows only span a
$d^2$-dimensional subspace of the space of observable probabilities. 

We can write $B$ as $B = B_0 \Pi$ where $\Pi$ is a $d^2 \times N_{f1}$ matrix and $B_0$ is a $d^2
\times d^2$ matrix. Here, $B_0$ determines \textit{only} the gauge, while $\Pi$ has a real effect --
we can choose $B_0$ arbitrarily, but not $\Pi$. To choose an optimal $\Pi$, first we rewrite out
$AB$ using Equation \ref{eq:min_i_tilde} as
\begin{equation}
    AB = \tilde{\mathbbm{1}} B^T (B B^T)^{-1} B = \tilde{\mathbbm{1}} \Pi^T \Pi
\end{equation}
and then defining the complement projector $\Pi_c = \mathbb{I} - \Pi^T \Pi$ gives
\begin{equation}
    |\tilde{\mathbbm{1}} - AB|^2 = \tr{\Pi_c \tilde{\mathbbm{1}}^T \tilde{\mathbbm{1}} \Pi_c}
\end{equation}
This is uniquely minimised by choosing $\Pi$ to be the projector onto the $d^2$ right singular
vectors of $\tilde{\mathbbm{1}}$ with the largest singular values. \textit{I think this is referring
to decomposing $\tilde{\mathbbm{1}} = U \Sigma V^*$ using SVD, and projecting onto the first $d^2$
vectors of $V^*$. I am not sure why this minimises $|\tilde{\mathbbm{1}} - AB|^2$.}
\todoin{Find out why $\tr{\Pi_c \tilde{\mathbbm{1}}^T \tilde{\mathbbm{1}} \Pi_c}$ is minimised by
using SVD here. But is it actually important?}

Subsituting $B = B_0 \Pi$ into the estimates for native states, gates, and effects yields
\begin{align}
    \superket{\rho^{(l)}} &= B_0 \left(\Pi \tilde{\mathbbm{1}}^T \tilde{\mathbbm{1}} \Pi^T \right)^{-1} \Pi \tilde{\mathbbm{1}}^T \vec{R}^{(l)} \\
    G_k                   &= B_0 \left(\Pi \tilde{\mathbbm{1}}^T \tilde{\mathbbm{1}} \Pi^T \right)^{-1} \left(\Pi \tilde{\mathbbm{1}}^T P_k \Pi^T \right) B_0^{-1} \\
    \superbra{E_l^{(m)}}  &= \left[\vec{Q}_l^{(m)}\right]^T \Pi^T B_0^{-1}
\end{align}

\subsubsection{Fiducial States}

We also assumed that informationally complete sets of fudicial states and effects were available.
\textit{Most processors admit just one native state preparation and measurement}. Therefore,
fiducial states and measurements must be implemented using gates from the gate set itself. To do
this, we define two sets of QI/QO fiducial circuits. Each fiducial state is prepared by applying one
of the preparation fiducial circuits to a native state, and each fiducial measurement is performed
by applying one of the measurement fiducial circuits before a native measurement. We can represent
this as
\begin{align}
    \superbra{E_i'}    &= \superbra{E_{t(i)}^{(m(i))}} \tau \left(H_{h(i)}\right) \\
    \superket{\rho_j'} &= \tau \left(F_{f(j)}\right) \superket{\rho^{r(j)}}
\end{align}
which looks very complicated. \textit{Recall that $\tau(S)$ is the transfer matrix of $S$}.
Notation:
\begin{itemize}
    \item $F_k$: preparation fiducial circuits
    \item $H_k$: measurement fiducial circuits
    \item $r(j)$: native preparation index
    \item $m(i)$: native measurement index
    \item $f(j), h(i)$: fiducial index
\end{itemize}
Consequences:
\begin{itemize}
    \item Every observable probability required can be obtained by running a specific circuit.
    \item It reduces the number of free parameters in the model, because fiducial states are not
    entirely independent.
    \item It places the burden of informational completeness on the choice of fiducial circuits,
    which requires the gates to be minimally erroneous. Errors can be checked by checking if the
    $d^2$ largest singular values of $\tilde{\mathbbm{1}}$ are sufficiently large.
\end{itemize}

\subsubsection{MLE}

\textit{This is more of an aside? Nothing concrete was given here, but might be worth mentioning 
in background chapter?}

\vspace{1em}

\noindent The LGST algorithms described aim to minimise the objective function
\begin{equation}
    f(\text{gate set}) = \sum_j \left(p_j^{\text{gate set}} - f_j^{\text{observed}}\right)^2
\end{equation}
but actually, not really. It doesn't minimise the average squared error over the entire dataset
because it doesn't take other observed probabilities into account, e.g. those in $P_k$. A better
objective function is the likelihood function
\begin{equation}
    \mathcal{L}(\text{gate set}) = \Pr (\text{data} | \text{gate set})
\end{equation}
\textit{Just know that least-squares does work, but MLE is better}.

\subsection{Long-Sequence Gate Set Tomography (LSGST)}

If the $A$ and $B$ matrices are well-conditioned -- having condition number $O(1)$ -- then each
element of $G_k$ is close to a linear combination of observed probabilities. The condition number
shows how strongly small changes in an input are magnified in the output. If each circuit is
performed $N$ times, then we know that
\begin{equation}
    \hat{p} = p \pm \frac{O(1)}{\sqrt{N}}
\end{equation}
where the accuracy is limited by SPAM noise. We can break this boundary by using \textit{deep
circuits}, where a gate may appear many times. e.g. The probabilities for
\begin{equation}
    \superbraket{E | G_k G_k G_k G_k | \rho}
\end{equation}
are four times as sensitive as $\superbraket{E | G_k | \rho}$, which therefore allows four times the
precision in estimating some aspects of $G_k$. This is the basis of \ac{LSGST}.

Broadly, \ac{LSGST} circuits have three parts:
\begin{enumerate}
    \item Prepare a state $\superket{\rho_k'}$ by performing a native operation followed by a
    fiducial circuit.
    \item Perform $p$ repetitions of a short circuit $g$.
    \item Perform a particular measurement $\superbra{E_i'^{(m)}}$ by performing a fiducial circuit
    and then a native \ac{POVM} measurement.
\end{enumerate}
which we can use to estimate probabilities like
\begin{equation}
    p = \superbraket{E_i'^{(m)} | \tau(g_j^p) | \rho_k'}
\end{equation}
We call the short circuit $g$ a \textit{germ} and $p$ the \textit{germ power}. The idea here is that
each repetition will amplify particular errors in $g$. e.g. Suppose we have $g = G$ where $G$ is a
single unitary gate which rotates by $\theta$; tomography on $g^p$ measures $p\theta$ to a precision
of $\pm \epsilon$, which means we have measured $\theta$ to a precision of $\pm \epsilon / p$. Two
kinds of parameters cannot be amplified: gauge parameters cannot be measured at all, and SPAM
operations cannot be amplified because they only occur once.

A set of germs which amplifies all possible variation in the gate set is called
\textit{amplificationally complete}. The set of germs which are amplificationally complete varies
depending on the target gate set, so they must be determined for each new gate set. \textit{A set of
germs $\{g_j\}$ is amplificationally complete iff. the union of the error subspaces amplified by
each $g_j$ span the complement of the subspace of gauge variations}. 

Once a set of germs is selected, a set of \textit{base circuits} must also be constructed. Each
circuit is then raised to several powers $p$. Using several powers prevents aliasing issues, e.g.
repeating an over-rotation of $\theta = \pi / 16$ by $p = 32$ would look like no error at all. What
is important here is the depth $l$ of the circuit, not the number of repetitions $p$ itself. The
optimal choices for $l$ are logarithmically spaced $l = 1, m, m^2, \dots$. Empirically, $m = 2$, i.e.
$l = 1, 2, 4, 8, \dots$ was found to be a good starting point, but others may work. The depth of a
``depth $l$'' circuit is actually approximately $l$: a germ of depth 5 would appear at first at $l =
8, p = 1$, then again at $l = 16, p = 3$. Note that the maximum circuit depth $L$ is configurable,
and should be carefully. Increasing $L$ yields more precision, but increases the time taken to
analyse the data. Increasing $L$ beyond a certain point is useless since decoherence and stochastic
errors will dominate. If the rate of decoherence is $\eta$, then little can be learned from circuits
of depth $L > O(1) / \eta$.

\vspace{1em}

\begin{equation} \label{eq:lsgst_probabilities}
    p_{abij} = \superbraket{E_{t(a)}^{m(a)} | \tau\left(H_{h(a)}\right) \tau(g_i)^{p_{i,j}} \tau\left(F_{f(b)}\right) | \rho^{r(b)}}
\end{equation}

\vspace{1em}

\noindent With these changes, \ac{LSGST} experiments are constructed as follows:
\begin{enumerate}
    \item Select a set of amplificationally complete germs.
    \item Select a set of base circuits given by $\mathcal{O} = \{g_i^{p_{i,j}}\}_{i,j}$ where $i$
    indexes a germ and $p_{i, j}$ is the $j$-th power applied to the $i$-th germ.
    \item Perform the circuits in \ref{eq:lsgst_probabilities} to obtain probabilities, which
    corresponds to:
    \begin{enumerate}
        \item Prepare the $r(b)$-th state.
        \item Perform the circuit $F_{f(b)} g_i^{p_{i, j}} H_{h(a)}$.
        \item Measure using the $m(a)$-th type of measurement.
    \end{enumerate}
    \item Estimate $p_{abij}$ with $\hat{p}_{abij} = f_{t(a)} = n_{t(a)} / N$, where $n_{t(a)}$ is
    the number of times the $t(a)$-th outcome was observed after repeating the circuit $N$ times.
\end{enumerate}

\subsubsection{Estimating Gate Set Parameters with LSGST}

Previously, we saw that MLE is a better choice of objective function over least-squares, although
we'll be using the log-likelihood since it is simpler. MLE answers the question ``given a set of
data, which set of parameters is most likely to produce it?'' To answer this, we must create a
\textit{model} for a gate set. $\mathcal{G}$ denotes a gate set model which maps a vector of
parameters $\vec{\theta}$ to concrete gate set. In other words, the gate set is decided entirely by
the set of parameters $\vec{\theta}$ and the choice of mapping. LSGST is agnostic to the choice of
model, but usually the model imposes constraints such as \ac{CPTP}. \textit{Would the parameters
here be the calibration of the Mach-Zehender interferometers}?

We're aiming to fit the data gathered from \ac{LSGST} experiments as shown in the previous section.
The log-likelihood of a single circuit is the multinomial likelihood for an $m_s$-outcome Bernoulli
scheme
\begin{equation}
    \log \mathcal{L}_s = N_s \sum_{\beta_s} f_{s, \beta_s} \log(p_{s, \beta_s})
\end{equation}
where 
\begin{itemize}
    \item $s$ indexes the circuits,
    \item $N_s$ is the total number of times circuit $s$ was repeated,
    \item $m_s$ is the number of outcomes of $s$,
    \item $N_{s, \beta_s}$ is the number of times outcome $\beta_s$ was observed,
    \item $p_{s, \beta_s}$ is the true probability \textit{predicted} by $\mathcal{G}$ of getting
    outcome $\beta_s$,
    \item $f_{s, \beta_s}$ is the observed frequency for outcome $\beta_s$.
\end{itemize}
For the entire experiment, we simply sum over all circuits
\begin{equation}
    \log \mathcal{L} = \sum_s \log \mathcal{L}_s = \sum_{s, \beta_s} N_s f_{s, \beta_s} \log(p_{s, \beta_s})
\end{equation}

Estimating the maximum likelihood is actually pretty hard, so a good `first guess' is important.
\ac{LGST} provides a good starting point for the parameters, which can then be refined by
incorporating the \ac{LSGST} data in order to maximise the log-likelihood. Broadly, it works as:
\begin{enumerate}
    \item Generate an \ac{LSGST} dataset $\mathcal{D}_0$.
    \item Calculate an initial set of parameters $\vec{\theta} \leftarrow \vec{\theta}_0$ using \ac{LGST}.
    \item For each circuit depth $L \in 1, m, m^2, \dots$
    \begin{enumerate}
        \item Take the subset $\mathcal{D} \subseteq \mathcal{D}_0$ corresponding to circuits whose
        germ-power has depth $\le L$.
        \item Update the current estimate $\vec{\theta}$ according to $\arg \min (\chi^2,
        \mathcal{G}, \mathcal{D}, \vec{\theta})$, which returns the local optimum for $\chi^2
        (\mathcal{G}(\vec{\theta}), \mathcal{D})$.
    \end{enumerate}
    \item Calculate the final estimate $\vec{\theta}$ according to $\arg \min (- \log \mathcal{L},
    \mathcal{G}, \mathcal{D}_0, \vec{\theta})$.
\end{enumerate}
The basic idea here is that each estimate gets closer and closer to the true MLE, whilst being much
easier to calculate. The $\chi^2$ statistic is a local approximation to the negative log-likelihood,
and can be computed faster.

\todoin{IM GOING TOO DEEP INTO THE THEORY!}